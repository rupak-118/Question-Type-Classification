{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Type Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "## Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re, nltk\n",
    "import gensim\n",
    "import codecs\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, average_precision_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test datasets can be downloaded from here : http://cogcomp.cs.illinois.edu/Data/QA/QC/\n",
    "\n",
    "The data files are renamed to training_dataset.txt and validation_dataset.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Reading the datasets\n",
    "f_train = open('training_dataset.txt', 'r+')\n",
    "f_test = open('validation_dataset.txt', 'r+')\n",
    "\n",
    "train = pd.DataFrame(f_train.readlines(), columns = ['Question'])\n",
    "test = pd.DataFrame(f_test.readlines(), columns = ['Question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "This problem can essentially be looked at as a text classification task with hierarchical labels - Coarse and Fine. The question type provided is in a hierarchical form and hence, we can either use the major class (e.g. DESC, ENTY, HUM etc.) to do Coarse classification or we can use the fine labels (e.g. 'ind', 'place', 'others' etc.) along with the major class labels to do a much detailed/fine-grained question-type classification. \n",
    "\n",
    "In this notebook, we look at both of them, with more focus on fine-grained classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separating text content (question) and target class/label\n",
    "train['QType'] = train.Question.apply(lambda x: x.split(' ', 1)[0])\n",
    "train['Question'] = train.Question.apply(lambda x: x.split(' ', 1)[1])\n",
    "train['QType-Coarse'] = train.QType.apply(lambda x: x.split(':')[0])\n",
    "train['QType-Fine'] = train.QType.apply(lambda x: x.split(':')[1])\n",
    "test['QType'] = test.Question.apply(lambda x: x.split(' ', 1)[0])\n",
    "test['Question'] = test.Question.apply(lambda x: x.split(' ', 1)[1])\n",
    "test['QType-Coarse'] = test.QType.apply(lambda x: x.split(':')[0])\n",
    "test['QType-Fine'] = test.QType.apply(lambda x: x.split(':')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>QType</th>\n",
       "      <th>QType-Coarse</th>\n",
       "      <th>QType-Fine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5452</td>\n",
       "      <td>5452</td>\n",
       "      <td>5452</td>\n",
       "      <td>5452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5381</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>How deep is a fathom ?\\n</td>\n",
       "      <td>HUM:ind</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>962</td>\n",
       "      <td>1250</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Question    QType QType-Coarse QType-Fine\n",
       "count                       5452     5452         5452       5452\n",
       "unique                      5381       50            6         47\n",
       "top     How deep is a fathom ?\\n  HUM:ind         ENTY        ind\n",
       "freq                           3      962         1250        962"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>QType</th>\n",
       "      <th>QType-Coarse</th>\n",
       "      <th>QType-Fine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>What is the state flower of Michigan ?\\n</td>\n",
       "      <td>DESC:def</td>\n",
       "      <td>DESC</td>\n",
       "      <td>def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>138</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Question     QType QType-Coarse  \\\n",
       "count                                        500       500          500   \n",
       "unique                                       500        42            6   \n",
       "top     What is the state flower of Michigan ?\\n  DESC:def         DESC   \n",
       "freq                                           1       123          138   \n",
       "\n",
       "       QType-Fine  \n",
       "count         500  \n",
       "unique         39  \n",
       "top           def  \n",
       "freq          123  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>QType</th>\n",
       "      <th>QType-Coarse</th>\n",
       "      <th>QType-Fine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5952</td>\n",
       "      <td>5952</td>\n",
       "      <td>5952</td>\n",
       "      <td>5952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5871</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>How deep is a fathom ?\\n</td>\n",
       "      <td>HUM:ind</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>1017</td>\n",
       "      <td>1344</td>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Question    QType QType-Coarse QType-Fine\n",
       "count                       5952     5952         5952       5952\n",
       "unique                      5871       50            6         47\n",
       "top     How deep is a fathom ?\\n  HUM:ind         ENTY        ind\n",
       "freq                           3     1017         1344       1017"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.append(test).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "1. 50 different question types/target labels are present in train+test combined\n",
    "2. For coarse classification, 6 question types/labels are present\n",
    "3. There are some duplicate questions in training set. None in test set\n",
    "4. 10 questions in test set are also present in training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding of target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(pd.Series(train.QType.tolist() + test.QType.tolist()).values)\n",
    "train['QType'] = le.transform(train.QType.values)\n",
    "test['QType'] = le.transform(test.QType.values)\n",
    "le2 = LabelEncoder()\n",
    "le2.fit(pd.Series(train['QType-Coarse'].tolist() + test['QType-Coarse'].tolist()).values)\n",
    "train['QType-Coarse'] = le2.transform(train['QType-Coarse'].values)\n",
    "test['QType-Coarse'] = le2.transform(test['QType-Coarse'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach :\n",
    "\n",
    "The general solution pipeline would be : \n",
    "\n",
    "Text pre-processing ----> Feature extraction ----> Training using an ML/DL algorithm ----> Parameter tuning, with cross-validation ----> Prediction and checking accuracy, precision etc, on test set \n",
    "\n",
    "We'll iterate through a lot of methods for pre-processing, feature extraction and ML/DL algos.\n",
    "\n",
    "Pre-processing : Text cleaning, stopword removal, stemming, lemmatization etc.\n",
    "\n",
    "Feature extraction/Word embeddings : Count Vectors, TF-IDF, GloVe, Word2Vec\n",
    "\n",
    "ML/DL algorithms : Linear models (Logistic Regression, Linear SVM), Non-linear models (Naive Bayes), Tree models (XGBoost, LightGBM), DL models (LSTMs/RNNs)\n",
    "\n",
    "Some points to note :\n",
    "1. In Question-type classification problems, there will be certain words which only appear\n",
    "   for certain classes, while they are absent in all other classes/qtypes. Extracting features\n",
    "   using TF-IDF should be ideal here as it penalizes words which occur in every example and lays\n",
    "   emphasis on certain words which appear only for specific training examples\n",
    "2. Identification using \"wh-\" words (Why, What, When etc.) might be useful. They provide some context, but\n",
    "   cannot be relied on too much\n",
    "3. n-gram methods and LSTM/RNNs can also be explored to derive information from the words appearing\n",
    "   frequently/sequentially\n",
    "4. The 50 target classes have hierarchical structure. An alternative approach can also be tried \n",
    "   later, wherein first, we try to predict the level 1 classes (DESC, HUM, NUM etc.) and then\n",
    "   again run the model for each of these classes to predict their sub-classes. This approach\n",
    "   would be highly inferable and easily explainable, but risks overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Importing required NLTK libraries\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing a combined corpus of train and test data\n",
    "all_corpus = pd.Series(train.Question.tolist() + test.Question.tolist()).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding words with stops in-between\n",
    "dot_words = []\n",
    "for row in all_corpus:\n",
    "    for word in row.split():\n",
    "        if '.' in word and len(word)>2:\n",
    "            dot_words.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'...': 1,\n",
       "         '...the': 1,\n",
       "         '.com': 1,\n",
       "         '.dbf': 1,\n",
       "         '.tbk': 1,\n",
       "         '1.76': 1,\n",
       "         '2.5': 1,\n",
       "         '42.3': 1,\n",
       "         '5.9': 1,\n",
       "         'A.G.': 1,\n",
       "         'Answers.com': 5,\n",
       "         'B.Y.O.B.': 1,\n",
       "         'Bros.': 2,\n",
       "         'C.C.': 1,\n",
       "         'D.A.': 1,\n",
       "         'D.B.': 1,\n",
       "         'D.C.': 6,\n",
       "         'D.H.': 1,\n",
       "         'Dr.': 4,\n",
       "         'G.M.T.': 1,\n",
       "         'H.G.': 1,\n",
       "         'I.V.': 1,\n",
       "         'Inc.': 3,\n",
       "         'J.D.': 1,\n",
       "         'J.F.K.': 1,\n",
       "         'J.R.R.': 2,\n",
       "         'Jan.': 1,\n",
       "         'Jr.': 4,\n",
       "         'KnowPost.com': 1,\n",
       "         'L.A.': 4,\n",
       "         'L.L.': 1,\n",
       "         'LL.M.': 1,\n",
       "         'Mr.': 6,\n",
       "         'Mrs.': 6,\n",
       "         'Ms.': 2,\n",
       "         'Mt.': 1,\n",
       "         'N.M': 1,\n",
       "         'No.': 3,\n",
       "         'No.1': 1,\n",
       "         'O.J.': 1,\n",
       "         'P.T.': 2,\n",
       "         'R.E.M.': 2,\n",
       "         'Rev.': 1,\n",
       "         'S.O.S.': 1,\n",
       "         'Sen.': 1,\n",
       "         'St.': 17,\n",
       "         'T.S.': 2,\n",
       "         'T.V.': 1,\n",
       "         'U.K.': 2,\n",
       "         'U.S': 1,\n",
       "         'U.S.': 151,\n",
       "         'U.S.-based': 1,\n",
       "         'U.S.A': 2,\n",
       "         'U.S.A.': 3,\n",
       "         'U.S.S.R.': 1,\n",
       "         'W.B.': 1,\n",
       "         'W.C.': 4,\n",
       "         'a.m.': 1,\n",
       "         'answers.com': 1,\n",
       "         'aol.com': 1,\n",
       "         'cc.': 1,\n",
       "         'creativity.': 1,\n",
       "         'cwt.': 1,\n",
       "         'discontent..': 1,\n",
       "         'e.g.': 2,\n",
       "         'etc.': 3,\n",
       "         'king..': 1,\n",
       "         'most...subversive': 1,\n",
       "         'name..': 1,\n",
       "         'p.m.': 3,\n",
       "         'q.i.d': 1,\n",
       "         'question..': 1,\n",
       "         'swift.': 1,\n",
       "         'v.9': 1,\n",
       "         'vs.': 3,\n",
       "         'www.answers.com': 1,\n",
       "         'www.questions.com': 1,\n",
       "         'yahoo.com': 1,\n",
       "         'year..': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(dot_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On exploring the corpus, it was found that there are some frequently-occuring words with punctuation marks within it. During text cleaning process, if we remove the punctuation marks, these words would be disintegrated and might lost its importance/discriminative ability during classification. Hence, we create a \"keep_list\" to ensure that frequently occuring words are not destroyed during text cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_dot_words = ['U.S.', 'St.', 'Mr.', 'Mrs.', 'D.C.'] # some frequently occuring words with punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_clean(corpus, keep_list):\n",
    "    '''\n",
    "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
    "    \n",
    "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
    "            even after the cleaning process\n",
    "    \n",
    "    Output : Returns the cleaned text corpus\n",
    "    \n",
    "    '''\n",
    "    cleaned_corpus = pd.Series()\n",
    "    for row in corpus:\n",
    "        qs = []\n",
    "        for word in row.split():\n",
    "            if word not in keep_list:\n",
    "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
    "                p1 = p1.lower()\n",
    "                qs.append(p1)\n",
    "            else : qs.append(word)\n",
    "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = True, remove_stopwords = True):\n",
    "    \n",
    "    '''\n",
    "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
    "    \n",
    "    Input : \n",
    "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
    "    'keep_list' - List of words to be retained during cleaning process\n",
    "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n",
    "                                                                  be performed or not\n",
    "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
    "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
    "    \n",
    "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
    "    \n",
    "    Output : Returns the processed text corpus\n",
    "    \n",
    "    '''\n",
    "    if cleaning == True:\n",
    "        corpus = text_clean(corpus, keep_list)\n",
    "    \n",
    "    if remove_stopwords == True:\n",
    "        stop = set(stopwords.words('english'))\n",
    "        corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
    "    else :\n",
    "        corpus = [[x for x in x.split()] for x in corpus]\n",
    "    \n",
    "    if lemmatization == True:\n",
    "        lem = WordNetLemmatizer()\n",
    "        corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
    "    \n",
    "    if stemming == True:\n",
    "        if stem_type == 'snowball':\n",
    "            stemmer = SnowballStemmer(language = 'english')\n",
    "            corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "        else :\n",
    "            stemmer = PorterStemmer()\n",
    "            corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    \n",
    "    corpus = [' '.join(x) for x in corpus]\n",
    "        \n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying the pre-processing function on the combined text corpus\n",
    "all_corpus = preprocess(all_corpus, keep_list = common_dot_words, remove_stopwords = False)\n",
    "\n",
    "# Separating back to train and test corpus\n",
    "trn_corpus = all_corpus[0:train.shape[0]]\n",
    "test_corpus = all_corpus[train.shape[0]::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction / Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feats_from_text(all_corpus, trn_corpus, test_corpus, n_dims = 500, model = 'tf-idf'):\n",
    "    \n",
    "    '''\n",
    "    Purpose : Function to extract numeric feature vectors/ word embeddings from the text corpus using one of CountVectors, tf-idf,\n",
    "              GloVe or Word2Vec\n",
    "    \n",
    "    Inputs : \n",
    "    'all_corpus' : Combined train + test corpus. The chosen model is fit on this corpus\n",
    "    'trn_corpus' and 'test_corpus' : Train and test corpora. The fitted word embedding is transformed over these two sets\n",
    "    'n_dims' : Gives the dimension of the transformed word vector. Default is set as 500 \n",
    "    'model' : Denotes which word embedding to use. Default is 'tf-idf'. Others are : 'cv', 'glove' and 'word2vec'\n",
    "    \n",
    "    Output : Returns two feature sets, 'train_feats' and 'test_feats'\n",
    "    \n",
    "    Note : The function returns an empty dataframe with an error message if an unknown model value is entered\n",
    "    \n",
    "    '''\n",
    "    ## TF-IDF\n",
    "    if model.lower() == 'tf-idf':\n",
    "        tfidf_vec = TfidfVectorizer(max_features = n_dims)\n",
    "        tfidf_vec.fit(all_corpus)\n",
    "        train_feats = tfidf_vec.transform(trn_corpus).toarray()\n",
    "        test_feats = tfidf_vec.transform(test_corpus).toarray()\n",
    "        # Converting feature-arrays to dataframes\n",
    "        train_feats = pd.DataFrame(train_feats)\n",
    "        train_feats.rename(columns = lambda x: 'w_'+str(x), inplace = True)\n",
    "        test_feats = pd.DataFrame(test_feats)\n",
    "        test_feats.rename(columns = lambda x: 'w_'+str(x), inplace = True)\n",
    "        \n",
    "    ## Count Vectors\n",
    "    if model.lower() == 'cv':\n",
    "        cv = CountVectorizer(max_features = n_dims)\n",
    "        cv.fit(all_corpus)\n",
    "        train_feats = cv.transform(trn_corpus).toarray()\n",
    "        test_feats = cv.transform(test_corpus).toarray()\n",
    "        # Converting feature-arrays to dataframes\n",
    "        train_feats = pd.DataFrame(train_feats)\n",
    "        train_feats.rename(columns = lambda x: 'w_'+str(x), inplace = True)\n",
    "        test_feats = pd.DataFrame(test_feats)\n",
    "        test_feats.rename(columns = lambda x: 'w_'+str(x), inplace = True)\n",
    "        \n",
    "    '''\n",
    "    In both, word2vec and GloVe, we take the mean of embeddings of all words in a question. Thus, now each question is \n",
    "    represented by a 300-D vector, whose values are an average of n 300-D vectors, each vector representing a word\n",
    "    in the sentence and as extracted from GloVe embeddings\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## GloVe\n",
    "    if model.lower() == 'glove':\n",
    "        glove_dict = {}\n",
    "        # Loading GloVe word embeddings\n",
    "        f = codecs.open('E:\\\\glove.6B.300d.txt', encoding = 'utf-8')\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            glove_dict[word] = coefs\n",
    "        f.close()\n",
    "        \n",
    "        # Tokenizing train and test corpus\n",
    "        trn_tokens = [[x for x in x.split(' ')]for x in trn_corpus]\n",
    "        test_tokens = [[x for x in x.split(' ')]for x in test_corpus]\n",
    "        \n",
    "        # Using mean word-->vector mappings stored in GloVe to prepare train and test features\n",
    "        train_feats = pd.DataFrame(np.array([np.mean([glove_dict[w] for w in words if w in glove_dict]\n",
    "                    or [np.zeros(300)], axis=0) for words in trn_tokens]))\n",
    "        \n",
    "        test_feats = pd.DataFrame(np.array([np.mean([glove_dict[w] for w in words if w in glove_dict]\n",
    "                    or [np.zeros(300)], axis=0) for words in test_tokens]))\n",
    "\n",
    "    \n",
    "    ## Word2Vec\n",
    "    if model.lower() == 'word2vec':\n",
    "        # Loading pre-trained word vectors\n",
    "        word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('E:\\\\GoogleNews-vectors-negative300.bin', binary = True)\n",
    "        w2v = dict(zip(word2vec_model.wv.index2word, word2vec_model.wv.syn0))\n",
    "        \n",
    "        # Tokenizing train and test corpus\n",
    "        trn_tokens = [[x for x in x.split(' ')]for x in trn_corpus]\n",
    "        test_tokens = [[x for x in x.split(' ')]for x in test_corpus]\n",
    "        \n",
    "        # Using mean word-->vector mappings stored in word2vec to prepare train and test features\n",
    "        train_feats = pd.DataFrame(np.array([np.mean([w2v[w] for w in words if w in w2v]\n",
    "                    or [np.zeros(300)], axis=0) for words in trn_tokens]))\n",
    "        \n",
    "        test_feats = pd.DataFrame(np.array([np.mean([w2v[w] for w in words if w in w2v]\n",
    "                    or [np.zeros(300)], axis=0) for words in test_tokens]))\n",
    "\n",
    "      \n",
    "    ## Error handling\n",
    "    if model.lower() not in ['tf-idf', 'cv', 'glove', 'word2vec']:\n",
    "        print(\"Unknown value for parameter 'model' entered. Returning an empty data-frame\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    return train_feats, test_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Feature extraction from text corpus\n",
    "train_feats, test_feats = feats_from_text(all_corpus, trn_corpus, test_corpus, model = 'tf-idf', n_dims = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating feature-sets (arrays)\n",
    "X = train_feats.values\n",
    "X_test = test_feats.values\n",
    "y = train.QType.values\n",
    "y_test = test.QType.values\n",
    "y_coarse = train['QType-Coarse'].values\n",
    "y_test_coarse = test['QType-Coarse'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML/DL algorithms for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Importing requisite libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "model = LogisticRegression(penalty = 'l2', C = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.67235189,  0.6533212 ,  0.66483012,  0.68888889,  0.6635514 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking cross-validation accuracy\n",
    "cross_val_score(model, X, y, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting model on entire training set\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making predictions on test set\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Linear SVM\n",
    "model = SVC(C = 1, kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking cross-validation accuracy\n",
    "cross_val_score(model, X, y, cv = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=6.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores for C = 0.1 are : [ 0.56713212  0.52705628  0.55786026  0.53318584  0.57174888  0.55141243]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=6.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores for C = 0.5 are : [ 0.6895811   0.67316017  0.67358079  0.65486726  0.69394619  0.6779661 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=6.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores for C = 1 are : [ 0.69924812  0.68181818  0.68122271  0.66371681  0.69618834  0.68474576]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=6.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores for C = 2 are : [ 0.70784103  0.68290043  0.68886463  0.67588496  0.70627803  0.69039548]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=6.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores for C = 5 are : [ 0.69709989  0.67099567  0.67139738  0.67035398  0.6838565   0.68813559]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=6.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores for C = 10 are : [ 0.70247046  0.64935065  0.66593886  0.66814159  0.67376682  0.67231638]\n",
      "\n",
      "[0.55139930067992837, 0.6771836002425663, 0.68448998787014093, 0.69202742595912115, 0.68030650354338762, 0.67199746151551187]\n"
     ]
    }
   ],
   "source": [
    "## Hyper-parameter tuning for SVC\n",
    "cv_means = []\n",
    "for i in [0.1, 0.5, 1, 2, 5, 10]:\n",
    "    model = SVC(C = i, kernel = 'linear')\n",
    "    cv_score = cross_val_score(model, X, y, cv = 6)\n",
    "    cv_means.append(cv_score.mean())\n",
    "    print(\"CV scores for C = {} are : {}\".format(i, cv_score))\n",
    "\n",
    "print(\"\\n{}\".format(cv_means))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " C = 2 turns out to be the best parameter. Hence, model is re-initialized accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting model on entire training set\n",
    "model = SVC(C = 2, kernel = 'linear')\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making predictions on test set\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian, Multinomial and Bernoulli Naive Bayes algorithms were tried. Multinomial NB was the most successful algorithm by a distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Multinomial Naive-Bayes\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.56642729,  0.55505005,  0.56014692,  0.57314815,  0.54205607])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking cross-validation accuracy\n",
    "cross_val_score(model, X, y, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting model\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Tree models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LGBMClassifier(boosting_type = 'gbdt', num_leaves = 31, max_depth = -1, reg_alpha = 0,\n",
    "                       reg_lambda = 0, learning_rate = 0.1, n_estimators = 1150, max_bin = 255, \n",
    "                       objective = 'multiclass', subsample = 1, subsample_freq = 1)\n",
    "## Model can be tuned further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=6.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting fold 1\n",
      "Accuracy for fold 1 = 0.6143931256713212\n",
      "\n",
      "Fitting fold 2\n",
      "Accuracy for fold 2 = 0.6182212581344902\n",
      "\n",
      "Fitting fold 3\n",
      "Accuracy for fold 3 = 0.6137855579868708\n",
      "\n",
      "Fitting fold 4\n",
      "Accuracy for fold 4 = 0.6172566371681416\n",
      "\n",
      "Fitting fold 5\n",
      "Accuracy for fold 5 = 0.6252796420581656\n",
      "\n",
      "Fitting fold 6\n",
      "Accuracy for fold 6 = 0.6110484780157835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross-validation framework\n",
    "fold_num = 1\n",
    "cv_acc = []\n",
    "skf = StratifiedKFold(n_splits = 6, shuffle = True, random_state = 102)\n",
    "for train_idx, val_idx in skf.split(X,y):\n",
    "    print(\"Fitting fold %d\" %fold_num)\n",
    "    model.fit(X[train_idx], y[train_idx], eval_metric = 'logloss', early_stopping_rounds = None)\n",
    "    cv_preds = model.predict(X[val_idx])\n",
    "    acc = accuracy_score(y[val_idx],cv_preds)\n",
    "    cv_acc.append(acc)\n",
    "    print(\"Accuracy for fold {} = {}\\n\".format(fold_num,acc))\n",
    "    fold_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1.0, learning_rate=0.1,\n",
       "        max_bin=255, max_depth=-1, min_child_samples=10,\n",
       "        min_child_weight=5, min_split_gain=0.0, n_estimators=1150,\n",
       "        n_jobs=-1, num_leaves=31, objective='multiclass', random_state=0,\n",
       "        reg_alpha=0, reg_lambda=0, silent=True, subsample=1,\n",
       "        subsample_for_bin=50000, subsample_freq=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting model on entire training set\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making predictions on test set\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(max_depth = 12, learning_rate = 0.01, n_estimators = 301, \n",
    "                      objective = \"multi:softmax\", gamma = 0, base_score = 0.5, \n",
    "                      reg_lambda = 10, subsample = 0.8, colsample_bytree = 0.8, num_class = 50)\n",
    "## Model can be tuned further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross-validation framework\n",
    "fold_num = 1\n",
    "cv_acc = []\n",
    "skf = StratifiedKFold(n_splits = 4, shuffle = True, random_state = 102)\n",
    "for train_idx, val_idx in skf.split(X,y):\n",
    "    print(\"Fitting fold %d\" %fold_num)\n",
    "    model.fit(X[train_idx], y[train_idx], eval_metric = \"merror\")\n",
    "    cv_preds = model.predict(X[val_idx])\n",
    "    acc = accuracy_score(y[val_idx], cv_preds)\n",
    "    cv_acc.append(acc)\n",
    "    print(\"Accuracy for fold {} = {}\\n\".format(fold_num,acc))\n",
    "    fold_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting model on entire training set\n",
    "model.fit(X, y, eval_metric = \"merror\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making predictions on test set\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Checking prediction accuracy on test set\n",
    "np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculating precision, recall etc. from confusion matrix\n",
    "cm_test = confusion_matrix(y_test, preds)\n",
    "precision = np.zeros(len(cm_test))\n",
    "recall = np.zeros(len(cm_test))\n",
    "for i in range(0, len(cm_test)):\n",
    "    precision[i] = cm_test[i,i]/(sum(cm_test[:,i]) + 10e-7)\n",
    "    recall[i] =  cm_test[i,i]/(sum(cm_test[i,:]) + 10e-7)\n",
    "\n",
    "print(\"Avg. precision and avg. recall are : {} and {} respectively\".format(np.mean(precision), np.mean(recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. To remove or not to remove (stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words starting with \"wh...\" like \"what\", \"why\", \"when\" etc. and some other words like \"how\" seem highly relevant intuitively for question-type classification problem. \n",
    "\n",
    "Removing stopwords removes all these \"wh-\" words. Not surprisingly, the performance metrics (multi-class accuracy, avg. precision, avg. recall) go down significantly on removal of these words.\n",
    "\n",
    "E.g. \n",
    "* MultinomialNB with 500 CountVector features - 36% test acc. and 48-49% val. acc.\n",
    "     \n",
    "* MultinomialNB with 500 CountVector features (no stopword removal) - 61-62% test and val acc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative is to remove all the stopwords, except the \"wh-\" words. This was tried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "stop = set(stopwords.words('english'))\n",
    "for word in wh_words:\n",
    "    stop.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However it was found, that the evaluation metrics were better when all stopwords were retained. \n",
    "\n",
    "* Linear SVM with 500 tf-idf features (complete non-removal of stopwords) : 73% val. acc. and 77% test acc.\n",
    "\n",
    "* Linear SVM with 500 tf-idf features (removal of stopwords, keeping only the wh_words) : 70% val acc. and 73% test acc.\n",
    "\n",
    "Hence, the final verdict is to NOT remove the stopwords at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Checking single and 2-letter words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During text cleaning process, one and two-letter words are generally removed. To inspect what kinds of words will be removed from our corpus if we perform this operation, we run the below line of code. This check is performed after removing the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "two_letter_words = [[x for x in x if len(x) <= 2] for x in all_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was observed that some of these words seemed important and having discriminative power, e.g. : numerals like 1, 11, 89 etc., common words like 'tv', 'cd', 'iq' etc. Numerals were central subject of quite a few questions. Hence, it was decided not to remove one or two-letter words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Coarse predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, the problem can also be approached by training the model on the 6 major classes instead of all the detailed 50 classes. Definitely, this will have a higher precision/recall/accuracy, but the biggest benefit is regarding the kind of errors the model would make. If we can optimize classification on these 6 major classes to a very high extent, then even if the model makes errors on fine classification, the errors would lie in an associated, similar hierarchical bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting model\n",
    "model.fit(X, y_coarse)\n",
    "# Making predictions\n",
    "model.predict(X_test)\n",
    "# Checking accuracy\n",
    "np.mean(preds == y_test_coarse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of coarse classifications :\n",
    "   1. Linear SVM with 500 TF-IDF features - 84% test acc., 80-81% val acc. + 87% and 83% test precision and recall respectively\n",
    "   2. Linear SVM with 500 CountVector features - 84% test acc., 79-80% val acc. + 88% and 83% test precision and recall \n",
    "      respectively\n",
    "   3. Multinomial NB with 500 TF-IDF features - 81% test acc., 75% val acc. + 85% and 79% test precision and recall respectively "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear models RULE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different kinds of models were tried with Count Vector and TF-IDF word embeddings. It was seen that the linear models (Logistic regression and SVM) performed much better than the others on this text classification task. SVM had the best validation accuracy. Performance metrics on different models are listed below.\n",
    "\n",
    "1. Linear SVM with 500 CountVector features - 78% test acc., 70% val acc. + 68% and 64% test precision and recall respectively\n",
    "\n",
    "2. Linear SVM with 500 TF-IDF features - 77% test acc., 73% val acc. + 67% and 63% test precision and recall respectively\n",
    "\n",
    "3. Logistic Regression with 500 CountVector features - 76% test acc., 70% val acc. + 65% and 56% test precision and recall\n",
    "\n",
    "4. MultinomialNB with 300 tf-idf features - 61% test acc. and 56% val acc.\n",
    "\n",
    "5. Untuned LGBM with 500 CountVector features - 64% test acc.\n",
    "\n",
    "6. Untuned XGBoost with 500 CountVector features - 68% test acc.\n",
    "\n",
    "\n",
    "If we think of it, text classification datasets are very well-suited for algorithms like SVM and matrix factorization methods. Due to the word embeddings like Count Vectors/TF-IDF, the feature-set created was sparse and suited for factorization-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. War of the Embeddings - TFIDF/CV vs GloVe/Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF and Count Vector embeddings generate sparse matrices with non-negative values, whereas GloVe and Word2Vec generate a dense matrix with all kinds of real values. As such, both kinds of embeddings are suited for different kinds of algorithms.\n",
    "\n",
    "Simple, linear models, matrix factorization methods, SVMs etc. favour sparse matrices and hence perform well along with TF-IDF/CountVector embeddings. GloVe and Word2Vec embeddings are more suited to complex models (e.g. GBM models, neural networks etc.). This was also seen when comparing their performance with different algorithms\n",
    "\n",
    " * TF-IDF/CV\n",
    "         1. Linear SVM with Count Vector - 78% test acc.\n",
    "         2. LGBM with Count Vector - 64% test acc.\n",
    "         \n",
    " * GloVe/Word2Vec\n",
    "         1. LGBM with pre-trained Word2Vec - 72% test acc.\n",
    "         2. Logistic Regression with Word2Vec - 70% test acc.\n",
    "         \n",
    "         3. LGBM with pre-trained GloVe - 74% test acc. (better precision and recall as compared to Word2Vec)\n",
    "         4. Logistic Regression with GloVe - 71% test acc.\n",
    "         \n",
    " \n",
    "In general, linear SVM performs well irrespective of the word embeddings. In fact, linear SVM with word2vec embeddings reach close to 77% test accuracy. SVM models also have higher precision/recall than other algorithms in either of the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, linear models, especially linear SVM seems most suited for this problem. Better feature engineering, combined with linear SVM should improve the models further. This also seems to be backed by the vast literature available for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more ideas/experiments which can be executed on this dataset to potentially improve score and/or to build diverse models are listed below :\n",
    "\n",
    "1. Ensembles - Lots of linear, tree, non-linear models were used in this solution. Being a diverse bunch of models, they form good candidates to be combined in an ensemble model (maybe, stacking)\n",
    "\n",
    "2. Tuning parameters of the Tree models - will bring a small gain in the metrics\n",
    "\n",
    "3. Using embeddings like word2vec and GloVe in a different way (not mean of words in a sentence) - maybe, the word embeddings can be weighted by TF-IDF, or mean of only few significant words in the sentence can be taken\n",
    "\n",
    "4. Using ANNs with word2vec/GloVe embeddings as the resultant feature-sets seem suited for deep, non-linear models\n",
    "\n",
    "5. Exploring with LSTMs/RNNs and treating this as a sequence problem. Didn't try LSTMs/RNNs here since linear models were performing far superiorly\n",
    "\n",
    "6. Using only first few words in each question to extract features - Usually, the first few words contain keywords like \"What\", \"What is\", \"Name\", \"How\" etc.\n",
    "\n",
    "7. Implementation of state-of-the-art methods from research papers (e.g. http://bit.ly/2CzrGE1, some of Stanford NLP group's work)\n",
    "\n",
    "8. The classic way of error analysis - Looking at examples which the model is getting wrong to derive insights as to how to improve precision/recall etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
